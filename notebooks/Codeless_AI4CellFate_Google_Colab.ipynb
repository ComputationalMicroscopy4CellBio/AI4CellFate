{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß¨ AI4CellFate: Interpretable Cell Fate Prediction\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ComputationalMicroscopy4CellBio/AI4CellFate/blob/main/notebooks/Codeless_AI4CellFate_Google_Colab.ipynb)\n",
        "\n",
        "**AI4CellFate** is a deep learning framework for predicting cell fate from single-frame microscopy images with full interpretability. This notebook provides an interface to train and apply AI4CellFate to your own data.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã **Quick Start Guide**\n",
        "\n",
        "### **Step 1: Prepare Your Data**\n",
        "- **Images**: Single-channel images (20√ó20 pixels recommended)\n",
        "- **Labels**: Binary classification labels (0 and 1)\n",
        "- **Format**: NumPy arrays (.npy files)\n",
        "- **‚ö†Ô∏è IMPORTANT**: Normalise your images to [0,1] range\n",
        "\n",
        "Then, execute the cells in order - the code is hidden for simplicity.\n",
        "\n",
        "### **Step 2: Upload Data**\n",
        "Upload your data files to this Colab session or connect to Google Drive\n",
        "\n",
        "### **Step 3: Train the Model**\n",
        "Run the cell that trains the AI4CellFate model.\n",
        "\n",
        "### **Step 4: Interpret Results**\n",
        "View latent space visualisations and feature interpretations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üîß **Setup and Installation** {display-mode: \"form\"}\n",
        "#@markdown This cell installs AI4CellFate and required dependencies. Run this first!\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Clone the AI4CellFate repository\n",
        "if not os.path.exists('AI4CellFate'):\n",
        "    !git clone https://github.com/ComputationalMicroscopy4CellBio/AI4CellFate.git\n",
        "    \n",
        "# Change to the repository directory\n",
        "os.chdir('AI4CellFate')\n",
        "sys.path.append('/content/AI4CellFate')\n",
        "\n",
        "# Install package-specific requirements\n",
        "%pip install -q tensorflow opencv-python scikit-learn matplotlib seaborn scipy\n",
        "\n",
        "# Import necessary modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import AI4CellFate modules\n",
        "from src.training.train import train_autoencoder, train_cellfate\n",
        "from src.models.classifier import mlp_classifier\n",
        "from src.models import Encoder, Decoder, Discriminator\n",
        "from src.preprocessing.preprocessing_functions import augment_dataset, augmentations\n",
        "\n",
        "print(\"‚úÖ AI4CellFate successfully installed and imported!\")\n",
        "print(\"üìä TensorFlow version:\", tf.__version__)\n",
        "#print(\"üî• GPU available:\", \"Yes\" if tf.config.list_physical_devices('GPU') else \"No\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üìÅ **Data Upload and Preparation** {display-mode: \"form\"}\n",
        "\n",
        "#@markdown ## üìã **Data Preparation Guidelines**\n",
        "#@markdown Before uploading, ensure your data meets these requirements:\n",
        "#@markdown\n",
        "#@markdown ### **‚úÖ Required Format:**\n",
        "#@markdown - **Images**: NumPy arrays (.npy) with shape `[n_samples, height, width]`\n",
        "#@markdown - **Labels**: NumPy arrays (.npy) with binary labels (0 and 1)\n",
        "#@markdown - **Normalisation**: Images must be normalised to [0, 1] range\n",
        "#@markdown - **Splits**: Provide separate train, validation, and test sets\n",
        "#@markdown - **Balance**: Training set should be balanced (equal samples per class)\n",
        "#@markdown - **Augmentation**: Training set should preferably be augmented (5x recommended)\n",
        "#@markdown\n",
        "#@markdown ### **üìÇ Expected Files:**\n",
        "#@markdown - `x_train.npy` / `x_train_aug.npy` (training images)\n",
        "#@markdown - `y_train.npy` / `y_train_aug.npy` (training labels)  \n",
        "#@markdown - `x_val.npy` (validation images)\n",
        "#@markdown - `y_val.npy` (validation labels)\n",
        "#@markdown - `x_test.npy` (test images)\n",
        "#@markdown - `y_test.npy` (test labels)\n",
        "#@markdown\n",
        "#@markdown ### **üîß Preprocessing Steps (Do Before Upload):**\n",
        "#@markdown 1. **Normalise** images: `images = images / images.max()` or `images = (images - images.min()) / (images.max() - images.min())`\n",
        "#@markdown 2. **Split** data: 60% train, 20% validation, 20% test (stratified)\n",
        "#@markdown 3. **Balance** training set: Equal number of samples per class\n",
        "#@markdown 4. **Augment** training set: Apply rotations, flips (5x augmentation recommended)\n",
        "#@markdown 5. **Select frame**: If time-lapse data, extract single frame per cell `[n_samples, height, width]`\n",
        "\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## üì§ **Upload Method**\n",
        "upload_method = \"drive\" #@param [\"drive\", \"upload\"]\n",
        "\n",
        "if upload_method == \"drive\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    #@markdown ### **üìÅ Google Drive File Paths**\n",
        "    #@markdown Specify the full paths to your prepared data files:\n",
        "    \n",
        "    x_train_path = \"/content/drive/MyDrive/AI4CellFate_data/x_train_aug.npy\" #@param {type:\"string\"}\n",
        "    y_train_path = \"/content/drive/MyDrive/AI4CellFate_data/y_train_aug.npy\" #@param {type:\"string\"}\n",
        "    x_val_path = \"/content/drive/MyDrive/AI4CellFate_data/x_val.npy\" #@param {type:\"string\"}\n",
        "    y_val_path = \"/content/drive/MyDrive/AI4CellFate_data/y_val.npy\" #@param {type:\"string\"}\n",
        "    x_test_path = \"/content/drive/MyDrive/AI4CellFate_data/x_test.npy\" #@param {type:\"string\"}\n",
        "    y_test_path = \"/content/drive/MyDrive/AI4CellFate_data/y_test.npy\" #@param {type:\"string\"}\n",
        "    \n",
        "    # Load data from Drive\n",
        "    print(\"üìÅ Loading data from Google Drive...\")\n",
        "    try:\n",
        "        x_train_aug = np.load(x_train_path)\n",
        "        y_train_aug = np.load(y_train_path)\n",
        "        x_val = np.load(x_val_path)\n",
        "        y_val = np.load(y_val_path)\n",
        "        x_test = np.load(x_test_path)\n",
        "        y_test = np.load(y_test_path)\n",
        "        print(\"‚úÖ All files loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading files: {e}\")\n",
        "        print(\"Please check your file paths and ensure files exist.\")\n",
        "        raise\n",
        "\n",
        "else:\n",
        "    # Upload files directly\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì§ Upload your prepared data files (.npy format)\")\n",
        "    print(\"Expected files: x_train_aug.npy, y_train_aug.npy, x_val.npy, y_val.npy, x_test.npy, y_test.npy\")\n",
        "    \n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    # Load uploaded files\n",
        "    try:\n",
        "        # Find and load files based on naming patterns\n",
        "        train_x_file = next((f for f in uploaded.keys() if 'train' in f.lower() and ('x_' in f.lower() or 'image' in f.lower())), None)\n",
        "        train_y_file = next((f for f in uploaded.keys() if 'train' in f.lower() and ('y_' in f.lower() or 'label' in f.lower())), None)\n",
        "        val_x_file = next((f for f in uploaded.keys() if 'val' in f.lower() and ('x_' in f.lower() or 'image' in f.lower())), None)\n",
        "        val_y_file = next((f for f in uploaded.keys() if 'val' in f.lower() and ('y_' in f.lower() or 'label' in f.lower())), None)\n",
        "        test_x_file = next((f for f in uploaded.keys() if 'test' in f.lower() and ('x_' in f.lower() or 'image' in f.lower())), None)\n",
        "        test_y_file = next((f for f in uploaded.keys() if 'test' in f.lower() and ('y_' in f.lower() or 'label' in f.lower())), None)\n",
        "        \n",
        "        if not all([train_x_file, train_y_file, val_x_file, val_y_file, test_x_file, test_y_file]):\n",
        "            missing = []\n",
        "            if not train_x_file: missing.append(\"training images\")\n",
        "            if not train_y_file: missing.append(\"training labels\")\n",
        "            if not val_x_file: missing.append(\"validation images\")\n",
        "            if not val_y_file: missing.append(\"validation labels\")\n",
        "            if not test_x_file: missing.append(\"test images\")\n",
        "            if not test_y_file: missing.append(\"test labels\")\n",
        "            raise ValueError(f\"Missing required files: {', '.join(missing)}\")\n",
        "        \n",
        "        x_train_aug = np.load(train_x_file)\n",
        "        y_train_aug = np.load(train_y_file)\n",
        "        x_val = np.load(val_x_file)\n",
        "        y_val = np.load(val_y_file)\n",
        "        x_test = np.load(test_x_file)\n",
        "        y_test = np.load(test_y_file)\n",
        "        \n",
        "        print(\"‚úÖ All files loaded successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading files: {e}\")\n",
        "        raise\n",
        "\n",
        "# Validate data format and requirements\n",
        "print(\"\\nüîç **Data Validation:**\")\n",
        "\n",
        "# Check shapes\n",
        "print(f\"üìä **Data Shapes:**\")\n",
        "print(f\"   ‚Ä¢ Training: {x_train_aug.shape} images, {y_train_aug.shape} labels\")\n",
        "print(f\"   ‚Ä¢ Validation: {x_val.shape} images, {y_val.shape} labels\")\n",
        "print(f\"   ‚Ä¢ Test: {x_test.shape} images, {y_test.shape} labels\")\n",
        "\n",
        "# Check if images are 3D (samples, height, width)\n",
        "if len(x_train_aug.shape) != 3:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Training images have shape {x_train_aug.shape}. Expected 3D: [samples, height, width]\")\n",
        "if len(x_val.shape) != 3:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Validation images have shape {x_val.shape}. Expected 3D: [samples, height, width]\")\n",
        "if len(x_test.shape) != 3:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Test images have shape {x_test.shape}. Expected 3D: [samples, height, width]\")\n",
        "\n",
        "# Check normalisation\n",
        "train_min, train_max = x_train_aug.min(), x_train_aug.max()\n",
        "val_min, val_max = x_val.min(), x_val.max()\n",
        "test_min, test_max = x_test.min(), x_test.max()\n",
        "\n",
        "print(f\"\\nüìè **Normalisation Check:**\")\n",
        "print(f\"   ‚Ä¢ Training: [{train_min:.3f}, {train_max:.3f}]\")\n",
        "print(f\"   ‚Ä¢ Validation: [{val_min:.3f}, {val_max:.3f}]\")\n",
        "print(f\"   ‚Ä¢ Test: [{test_min:.3f}, {test_max:.3f}]\")\n",
        "\n",
        "if not (0 <= train_min and train_max <= 1 and 0 <= val_min and val_max <= 1 and 0 <= test_min and test_max <= 1):\n",
        "    print(\"‚ùå WARNING: Images not properly normalised to [0,1] range!\")\n",
        "    print(\"Please normalise your images before proceeding.\")\n",
        "else:\n",
        "    print(\"‚úÖ Images properly normalised to [0,1] range\")\n",
        "\n",
        "# Check class balance in training set\n",
        "unique_train, counts_train = np.unique(y_train_aug, return_counts=True)\n",
        "print(f\"\\n‚öñÔ∏è  **Training Set Balance:**\")\n",
        "for class_label, count in zip(unique_train, counts_train):\n",
        "    print(f\"   ‚Ä¢ Class {class_label}: {count} samples\")\n",
        "\n",
        "if len(counts_train) == 2 and abs(counts_train[0] - counts_train[1]) / max(counts_train) < 0.1:\n",
        "    print(\"‚úÖ Training set is balanced\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Training set appears unbalanced. Consider balancing before training.\")\n",
        "\n",
        "# Check binary labels\n",
        "all_labels = np.concatenate([y_train_aug, y_val, y_test])\n",
        "unique_labels = np.unique(all_labels)\n",
        "if not np.array_equal(unique_labels, [0, 1]):\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Labels contain values {unique_labels}. Expected binary labels [0, 1]\")\n",
        "else:\n",
        "    print(\"‚úÖ Binary labels [0, 1] detected\")\n",
        "\n",
        "print(f\"\\nüéØ **Data loaded and validated! Ready for training.**\")\n",
        "print(f\"\\nüìã **Variables available for training:**\")\n",
        "print(f\"   ‚Ä¢ x_train_aug: {x_train_aug.shape}\")\n",
        "print(f\"   ‚Ä¢ y_train_aug: {y_train_aug.shape}\")\n",
        "print(f\"   ‚Ä¢ x_val: {x_val.shape}\")\n",
        "print(f\"   ‚Ä¢ y_val: {y_val.shape}\")\n",
        "print(f\"   ‚Ä¢ x_test: {x_test.shape}\")\n",
        "print(f\"   ‚Ä¢ y_test: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìê **Image Size Requirements & Model Adaptation**\n",
        "\n",
        "### **Default Configuration**\n",
        "AI4CellFate is designed for **20x20 pixel grayscale images**. If your data matches this size, you can proceed directly to the next cell.\n",
        "\n",
        "### **For Different Image Sizes**\n",
        "If your images are larger (e.g., 64x64, 128x128), you'll need to modify the model architecture:\n",
        "\n",
        "#### **Step-by-Step Guide:**\n",
        "\n",
        "1. **Navigate to Model Files:**\n",
        "   ```\n",
        "   AI4CellFate/\n",
        "   ‚îî‚îÄ‚îÄ src/\n",
        "       ‚îî‚îÄ‚îÄ models/\n",
        "           ‚îú‚îÄ‚îÄ encoder.py    ‚Üê Modify this\n",
        "           ‚îî‚îÄ‚îÄ decoder.py    ‚Üê Modify this\n",
        "   ```\n",
        "\n",
        "2. **Update Encoder Architecture (`src/models/encoder.py`):**\n",
        "   - Locate the `Conv2D` layers in the `__init__` method\n",
        "   - **Add more layers** for larger images:\n",
        "     - For 64x64: Add 1-2 additional Conv2D layers\n",
        "     - For 128x128: Add 2-3 additional Conv2D layers\n",
        "   - **Pattern**: Each Conv2D layer typically halves the spatial dimensions\n",
        "   - **Example**: 128x128 ‚Üí 64x64 ‚Üí 32x32 ‚Üí 16x16 ‚Üí 8x8 ‚Üí 4x4 ‚Üí flatten\n",
        "\n",
        "3. **Update Decoder Architecture (`src/models/decoder.py`):**\n",
        "   - Mirror the encoder changes in reverse\n",
        "   - Adjust the `Dense` layer input size to match encoder output\n",
        "   - Use `Conv2DTranspose` layers to upscale back to original size\n",
        "\n",
        "4. **Key Parameters to Adjust:**\n",
        "   - `filters`: Number of feature maps (typically 32, 64, 128, 256)\n",
        "   - `kernel_size`: Usually (3,3) or (4,4)\n",
        "   - `strides`: Usually (2,2) for downsampling/upsampling\n",
        "   - `padding`: Usually 'same' to maintain dimensions\n",
        "\n",
        "#### **Quick Tips:**\n",
        "- **Maintain symmetry**: Encoder downsampling should match decoder upsampling\n",
        "- **Test incrementally**: Start with one additional layer pair\n",
        "- **Monitor memory**: Larger images require more GPU memory\n",
        "- **Adjust batch size**: You may need to reduce `batch_size` for larger images\n",
        "\n",
        "#### **Example Modification:**\n",
        "For 64x64 images, add one more Conv2D layer in encoder:\n",
        "```python\n",
        "# In encoder.py, add after existing Conv2D layers:\n",
        "x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)  # 64‚Üí32\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "```\n",
        "\n",
        "And corresponding Conv2DTranspose in decoder:\n",
        "```python\n",
        "# In decoder.py, add before existing Conv2DTranspose layers:\n",
        "x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(x)  # 32‚Üí64\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ‚öôÔ∏è **Model Configuration** {display-mode: \"form\"}\n",
        "#@markdown Configure AI4CellFate training parameters\n",
        "\n",
        "# Training parameters\n",
        "latent_dim = 2 #@param {type:\"integer\"}\n",
        "batch_size = 30 #@param {type:\"integer\"}\n",
        "stage1_epochs = 35 #@param {type:\"integer\"}\n",
        "stage2_epochs = 100 #@param {type:\"integer\"}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "random_seed = 42 #@param {type:\"integer\"}\n",
        "\n",
        "# Advanced parameters\n",
        "gaussian_noise_std = 0.003 #@param {type:\"number\"}\n",
        "lambda_recon_stage1 = 5 #@param {type:\"integer\"}\n",
        "lambda_adv_stage1 = 1 #@param {type:\"integer\"}\n",
        "lambda_recon_stage2 = 6 #@param {type:\"integer\"}\n",
        "lambda_adv_stage2 = 4 #@param {type:\"integer\"}\n",
        "lambda_cov = 1 #@param {type:\"number\"}\n",
        "lambda_contra = 8 #@param {type:\"integer\"}\n",
        "\n",
        "# Configuration dictionaries\n",
        "config_stage1 = {\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': stage1_epochs,\n",
        "    'learning_rate': learning_rate,\n",
        "    'seed': random_seed,\n",
        "    'latent_dim': latent_dim,\n",
        "    'GaussianNoise_std': gaussian_noise_std,\n",
        "    'lambda_recon': lambda_recon_stage1,\n",
        "    'lambda_adv': lambda_adv_stage1,\n",
        "}\n",
        "\n",
        "config_stage2 = {\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': stage2_epochs,\n",
        "    'learning_rate': learning_rate,\n",
        "    'seed': random_seed,\n",
        "    'latent_dim': latent_dim,\n",
        "    'GaussianNoise_std': gaussian_noise_std,\n",
        "    'lambda_recon': lambda_recon_stage2,\n",
        "    'lambda_adv': lambda_adv_stage2,\n",
        "    'lambda_cov': lambda_cov,\n",
        "    'lambda_contra': lambda_contra,\n",
        "}\n",
        "\n",
        "print(f\"‚öôÔ∏è **Model Configuration:**\")\n",
        "print(f\"   ‚Ä¢ Latent dimensions: {latent_dim}\")\n",
        "print(f\"   ‚Ä¢ Batch size: {batch_size}\")\n",
        "print(f\"   ‚Ä¢ Stage 1 epochs: {stage1_epochs}\")\n",
        "print(f\"   ‚Ä¢ Stage 2 epochs: {stage2_epochs}\")\n",
        "print(f\"   ‚Ä¢ Learning rate: {learning_rate}\")\n",
        "print(f\"   ‚Ä¢ Random seed: {random_seed}\")\n",
        "print(f\"\\nüîß **Advanced Parameters:**\")\n",
        "print(f\"   ‚Ä¢ Gaussian noise std: {gaussian_noise_std}\")\n",
        "print(f\"   ‚Ä¢ Lambda reconstruction (S1/S2): {lambda_recon_stage1}/{lambda_recon_stage2}\")\n",
        "print(f\"   ‚Ä¢ Lambda adversarial (S1/S2): {lambda_adv_stage1}/{lambda_adv_stage2}\")\n",
        "print(f\"   ‚Ä¢ Lambda covariance: {lambda_cov}\")\n",
        "print(f\"   ‚Ä¢ Lambda contrastive: {lambda_contra}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model configuration completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üöÄ **AI4CellFate Model Training (Stage 1 + Stage 2)** {display-mode: \"form\"}\n",
        "#@markdown Train the complete AI4CellFate model in two stages: (1) Adversarial Autoencoder, (2) Latent Space Engineering\n",
        "\n",
        "print(\"üöÄ **AI4CellFate Model Training**\")\n",
        "print(\"   Training will proceed in two stages:\")\n",
        "print(\"   ‚Ä¢ Stage 1: Adversarial Autoencoder (reconstruction + Gaussian latent space)\")\n",
        "print(\"   ‚Ä¢ Stage 2: Latent Space Engineering (+ covariance + contrastive losses)\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STAGE 1: Adversarial Autoencoder Training\n",
        "# =============================================================================\n",
        "print(\"üîÑ **Stage 1: Training Adversarial Autoencoder**\")\n",
        "print(\"   Learning basic image reconstruction and Gaussian latent space...\")\n",
        "\n",
        "stage1_results = train_autoencoder(\n",
        "    config_stage1, \n",
        "    x_train_aug, \n",
        "    x_val\n",
        ")\n",
        "\n",
        "# Extract trained models\n",
        "encoder = stage1_results['encoder']\n",
        "decoder = stage1_results['decoder']\n",
        "discriminator = stage1_results['discriminator']\n",
        "\n",
        "print(\"\\n‚úÖ **Stage 1 Completed!**\")\n",
        "print(f\"   ‚Ä¢ Final reconstruction loss: {stage1_results['recon_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final adversarial loss: {stage1_results['adv_loss'][-1]:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STAGE 2: AI4CellFate Training (Latent Space Engineering)\n",
        "# =============================================================================\n",
        "print(\"\\nüîÑ **Stage 2: AI4CellFate Training (Latent Space Engineering)**\")\n",
        "print(\"   Adding covariance and contrastive losses for feature disentanglement...\")\n",
        "\n",
        "stage2_results = train_cellfate(\n",
        "    config_stage2,\n",
        "    encoder,\n",
        "    decoder, \n",
        "    discriminator,\n",
        "    x_train_aug,\n",
        "    y_train_aug,\n",
        "    x_val,\n",
        "    y_val,\n",
        "    x_test,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "# Update models with final trained versions\n",
        "encoder = stage2_results['encoder']\n",
        "decoder = stage2_results['decoder']\n",
        "discriminator = stage2_results['discriminator']\n",
        "final_confusion_matrix = stage2_results['confusion_matrix']\n",
        "\n",
        "print(\"\\nüéâ **Stage 2 Completed!**\")\n",
        "print(f\"   ‚Ä¢ Final reconstruction loss: {stage2_results['recon_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final adversarial loss: {stage2_results['adv_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final covariance loss: {stage2_results['cov_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final contrastive loss: {stage2_results['contra_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Training stopped at epochs: {stage2_results['good_conditions_stop']}\")\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL RESULTS SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\nüèÜ **Final Training Results:**\")\n",
        "print(\"   ‚Ä¢ Models trained successfully on both stages\")\n",
        "print(\"   ‚Ä¢ Latent space engineered for optimal feature disentanglement\")\n",
        "print(\"   ‚Ä¢ Ready for latent space visualization and interpretation\")\n",
        "\n",
        "# Display final confusion matrix\n",
        "if final_confusion_matrix is not None:\n",
        "    print(f\"\\nüìä **Final Classification Performance:**\")\n",
        "    print(f\"   ‚Ä¢ Class 0 accuracy: {final_confusion_matrix[0,0]:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Class 1 accuracy: {final_confusion_matrix[1,1]:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Mean diagonal accuracy: {np.mean(np.diag(final_confusion_matrix)):.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ **AI4CellFate training completed successfully!**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üìä **Latent Space Visualization** {display-mode: \"form\"}\n",
        "#@markdown This cell visualizes the 2D latent space learned by AI4CellFate, showing how different cell fates are separated.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Predict the latent representations\n",
        "latent_2d = encoder.predict(x_train_aug) \n",
        "\n",
        "# Find extreme points for axis limits\n",
        "x_min, x_max = latent_2d[:, 0].min() - 0.5, latent_2d[:, 0].max() + 0.5\n",
        "y_min, y_max = latent_2d[:, 1].min() - 0.5, latent_2d[:, 1].max() + 0.5\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "# Scatter plot for each class separately with thin gray edges\n",
        "plt.scatter(latent_2d[y_train_aug == 0][:, 0], latent_2d[y_train_aug == 0][:, 1], \n",
        "            color='#648fff', label=\"Fate 0\", alpha=1, edgecolors='k', linewidth=0.5, rasterized=True)  \n",
        "plt.scatter(latent_2d[y_train_aug == 1][:, 0], latent_2d[y_train_aug == 1][:, 1], \n",
        "            color='#dc267f', label=\"Fate 1\", alpha=1, edgecolors='k', linewidth=0.5, rasterized=True)  \n",
        "\n",
        "# Set axis limits\n",
        "# plt.xlim(-3, 3)\n",
        "# plt.ylim(-3, 3)\n",
        "\n",
        "# Make tick labels much bigger and set to Arial font\n",
        "plt.tick_params(axis='both', which='major', labelsize=16)\n",
        "ax = plt.gca()\n",
        "for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
        "    label.set_fontname('Arial')\n",
        "\n",
        "# Increase font size and set Arial font\n",
        "plt.xlabel(\"Latent Feature 0 (z0)\", fontsize=18, fontname=\"Arial\")\n",
        "plt.ylabel(\"Latent Feature 1 (z1)\", fontsize=18, fontname=\"Arial\")\n",
        "plt.title(\"Latent Space\", fontsize=20, fontname=\"Arial\")\n",
        "\n",
        "# Legend and grid\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "#plt.savefig(\"latent_space.eps\", format=\"eps\", dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ **Latent Space Analysis:**\")\n",
        "print(f\"   ‚Ä¢ Total samples plotted: {len(latent_2d)}\")\n",
        "print(f\"   ‚Ä¢ Fate 0 samples: {np.sum(y_train_aug == 0)}\")\n",
        "print(f\"   ‚Ä¢ Fate 1 samples: {np.sum(y_train_aug == 1)}\")\n",
        "print(f\"   ‚Ä¢ Latent Feature 0 range: [{latent_2d[:, 0].min():.3f}, {latent_2d[:, 0].max():.3f}]\")\n",
        "print(f\"   ‚Ä¢ Latent Feature 1 range: [{latent_2d[:, 1].min():.3f}, {latent_2d[:, 1].max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üîç **Latent Feature Interpretation** {display-mode: \"form\"}\n",
        "#@markdown This cell generates synthetic images by perturbing individual latent features to understand what each feature controls.\n",
        "\n",
        "#@markdown ### **Interpretation Parameters**\n",
        "feature_index = 0 #@param {type:\"slider\", min:0, max:1, step:1}\n",
        "#@markdown Choose which latent feature to interpret (0 or 1)\n",
        "\n",
        "perturbation_min = -2.0 #@param {type:\"slider\", min:-3.0, max:0.0, step:0.1}\n",
        "perturbation_max = 2.0 #@param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
        "#@markdown Set the range for perturbations\n",
        "\n",
        "num_steps = 5 #@param {type:\"slider\", min:3, max:10, step:1}\n",
        "#@markdown Number of perturbation steps to visualize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(f\"üéØ **Interpreting Latent Feature {feature_index}**\")\n",
        "print(f\"   ‚Ä¢ Perturbation range: [{perturbation_min:.1f}, {perturbation_max:.1f}]\")\n",
        "print(f\"   ‚Ä¢ Number of steps: {num_steps}\")\n",
        "\n",
        "# Create baseline latent vector\n",
        "baseline_latent_vector = np.zeros((2, 2), dtype=np.float32)  # Start with a neutral latent vector\n",
        "\n",
        "# Perturbation range\n",
        "perturbations = np.linspace(perturbation_min, perturbation_max, num_steps)\n",
        "\n",
        "# Store the perturbed reconstructions\n",
        "perturbed_reconstructions = []\n",
        "\n",
        "print(f\"\\nüîÑ Generating synthetic images...\")\n",
        "for i, value in enumerate(perturbations):\n",
        "    # Create a copy of the baseline latent vector\n",
        "    perturbed_vector = baseline_latent_vector.copy()\n",
        "    \n",
        "    # Modify the selected feature\n",
        "    perturbed_vector[0, feature_index] = value\n",
        "    \n",
        "    # Decode the perturbed vector to generate a synthetic image\n",
        "    synthetic_image = decoder.predict(perturbed_vector, verbose=0)  # Assuming 'decoder' is your trained decoder model\n",
        "    \n",
        "    # Store the result\n",
        "    perturbed_reconstructions.append(synthetic_image[0])  # Assuming decoder outputs (batch_size, height, width, channels)\n",
        "    print(f\"   ‚Ä¢ Step {i+1}/{num_steps}: Perturbation value {value:.2f}\")\n",
        "\n",
        "# Convert list to numpy array for easier handling\n",
        "perturbed_reconstructions = np.array(perturbed_reconstructions)\n",
        "\n",
        "# Plot the results\n",
        "fig, axs = plt.subplots(1, num_steps, figsize=(4*num_steps, 4))\n",
        "\n",
        "# Handle single subplot case\n",
        "if num_steps == 1:\n",
        "    axs = [axs]\n",
        "\n",
        "vmin = perturbed_reconstructions.min()\n",
        "vmax = perturbed_reconstructions.max()\n",
        "\n",
        "for i in range(num_steps):\n",
        "    im = axs[i].imshow(perturbed_reconstructions[i, :, :, 0], cmap='gray', vmin=0.25, vmax=vmax)\n",
        "    axs[i].set_title(f'z{feature_index} = {perturbations[i]:.2f}', fontsize=14, fontname='Arial')\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.suptitle(f'Latent Feature {feature_index} Interpretation', fontsize=16, fontname='Arial', y=1.02)\n",
        "plt.tight_layout()\n",
        "#plt.savefig(f\"perturbations_feat{feature_index}.eps\", format=\"eps\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ **Feature Interpretation Complete!**\")\n",
        "print(f\"   ‚Ä¢ Generated {num_steps} synthetic images\")\n",
        "print(f\"   ‚Ä¢ Feature {feature_index} controls: [Observe the visual changes across perturbations]\")\n",
        "print(f\"   ‚Ä¢ Use different feature_index values to interpret other latent dimensions\")\n",
        "print(f\"\\nüí° **Interpretation Tips:**\")\n",
        "print(f\"   ‚Ä¢ Smooth changes = well-learned feature representation\")\n",
        "print(f\"   ‚Ä¢ Abrupt changes = potential feature entanglement\")\n",
        "print(f\"   ‚Ä¢ Try different perturbation ranges to explore feature sensitivity\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
