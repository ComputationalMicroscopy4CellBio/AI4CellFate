{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß¨ AI4CellFate: Interpretable Cell Fate Prediction\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ComputationalMicroscopy4CellBio/AI4CellFate/blob/main/notebooks/Codeless_AI4CellFate.ipynb)\n",
        "\n",
        "**AI4CellFate** is a deep learning framework for predicting cell fate from single-frame microscopy images with full interpretability. This notebook provides an interface to train and apply AI4CellFate to your own data.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã **Quick Start Guide**\n",
        "\n",
        "### **Step 1: Prepare Your Data**\n",
        "- **Images**: Single-channel images (20√ó20 pixels recommended)\n",
        "- **Labels**: Binary classification labels (0 and 1)\n",
        "- **Format**: NumPy arrays (.npy files)\n",
        "- **‚ö†Ô∏è IMPORTANT**: Normalise your images to [0,1] range\n",
        "\n",
        "### **Step 2: Upload Data**\n",
        "Upload your data files to this Colab session or connect to Google Drive\n",
        "\n",
        "### **Step 3: Run the Cells**\n",
        "Execute the cells in order - the code is hidden for simplicity\n",
        "\n",
        "### **Step 4: Interpret Results**\n",
        "View latent space visualizations and feature interpretations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üîß **Setup and Installation** {display-mode: \"form\"}\n",
        "#@markdown This cell installs AI4CellFate and required dependencies. Run this first!\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Clone the AI4CellFate repository\n",
        "if not os.path.exists('AI4CellFate'):\n",
        "    !git clone https://github.com/ComputationalMicroscopy4CellBio/AI4CellFate.git\n",
        "    \n",
        "# Change to the repository directory\n",
        "os.chdir('AI4CellFate')\n",
        "sys.path.append('/content/AI4CellFate')\n",
        "\n",
        "# Install package-specific requirements\n",
        "%pip install -q -r requirements.txt\n",
        "\n",
        "# Import necessary modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import AI4CellFate modules\n",
        "from src.training.train import train_autoencoder, train_cellfate\n",
        "from src.models.classifier import mlp_classifier\n",
        "from src.models import Encoder, Decoder, Discriminator\n",
        "from src.preprocessing.preprocessing_functions import augment_dataset, augmentations\n",
        "\n",
        "print(\"‚úÖ AI4CellFate successfully installed and imported!\")\n",
        "print(\"üìä TensorFlow version:\", tf.__version__)\n",
        "#print(\"üî• GPU available:\", \"Yes\" if tf.config.list_physical_devices('GPU') else \"No\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üìÅ **Interactive Data Upload, Splitting, Augmentation and Normalisation** {display-mode: \"form\"}\n",
        "#@markdown Use this cell to upload data; split if needed (check which boxes apply to your data - if you haven't split your data, click on \"no_splits_available\"); pick a time frame which you want to predict from; balance/augment train (click on the \"has_augmented_train_set\" if you already have an augmented train set); and ensure normalisation to [0,1].\n",
        "\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Split availability options\n",
        "# -----------------------------\n",
        "print(\"üîß Data availability options\")\n",
        "has_train_split = True  #@param {type:\"boolean\"}\n",
        "has_val_split = True   #@param {type:\"boolean\"}\n",
        "has_test_split = True   #@param {type:\"boolean\"}\n",
        "no_splits_available = False  #@param {type:\"boolean\"}\n",
        "\n",
        "if no_splits_available:\n",
        "    has_train_split = False\n",
        "    has_val_split = False\n",
        "    has_test_split = False\n",
        "\n",
        "# -----------------------------\n",
        "# Augmentation settings (set before data upload)\n",
        "# -----------------------------\n",
        "has_augmented_train_set = False  #@param {type:\"boolean\"}\n",
        "augment_times = 5  #@param {type:\"integer\"}\n",
        "# -----------------------------\n",
        "# Upload method\n",
        "# -----------------------------\n",
        "upload_method = \"upload\" #@param [\"upload\", \"drive\"]\n",
        "\n",
        "#@markdown **Fill these paths in case you chose the \"drive\" option for data uploading:**\n",
        "\n",
        "train_images_path = \"/content/drive/MyDrive/final_split_ai4cellfate_data/augmented_x_train.npy\" #@param {type:\"string\"}\n",
        "train_labels_path = \"/content/drive/MyDrive/final_split_ai4cellfate_data/augmented_y_train.npy\" #@param {type:\"string\"}\n",
        "val_images_path = \"/content/drive/MyDrive/final_split_ai4cellfate_data/x_val.npy\" #@param {type:\"string\"}\n",
        "val_labels_path = \"/content/drive/MyDrive/final_split_ai4cellfate_data/y_val.npy\" #@param {type:\"string\"}\n",
        "test_images_path = \"/content/drive/MyDrive/final_split_ai4cellfate_data/x_test.npy\" #@param {type:\"string\"}\n",
        "test_labels_path = \"/content/drive/MyDrive/final_split_ai4cellfate_data/y_test.npy\" #@param {type:\"string\"}\n",
        "combined_images_path = \"\" #@param {type:\"string\"}\n",
        "combined_labels_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load data depending on splits\n",
        "# -----------------------------\n",
        "X_train_raw = y_train_raw = X_val_raw = y_val_raw = X_test_raw = y_test_raw = None\n",
        "all_uploaded_files = []\n",
        "\n",
        "if not has_train_split and not has_val_split and not has_test_split:\n",
        "    # Single combined dataset\n",
        "    if upload_method == \"upload\":\n",
        "        print(\"üì§ Upload all your files (images and labels). You can click 'Choose Files' multiple times to upload from different locations.\")\n",
        "        print(\"Expected file naming: files should contain 'image'/'x_' for images and 'label'/'y_' for labels\")\n",
        "        \n",
        "        # Allow multiple uploads\n",
        "        upload_done = False\n",
        "        while not upload_done:\n",
        "            try:\n",
        "                uploaded = files.upload()\n",
        "                all_uploaded_files.extend(list(uploaded.keys()))\n",
        "                print(f\"üì• Files uploaded so far: {all_uploaded_files}\")\n",
        "                \n",
        "                # Check if we have what we need\n",
        "                images_file = next((k for k in all_uploaded_files if ('image' in k.lower() or 'x_' in k.lower())), None)\n",
        "                labels_file = next((k for k in all_uploaded_files if ('label' in k.lower() or 'y_' in k.lower())), None)\n",
        "                \n",
        "                if images_file and labels_file:\n",
        "                    user_input = input(\"‚úÖ Found images and labels files. Type 'done' to proceed or press Enter to upload more files: \")\n",
        "                    if user_input.lower() == 'done':\n",
        "                        upload_done = True\n",
        "                else:\n",
        "                    user_input = input(\"‚è≥ Still need images and labels files. Press Enter to upload more files or type 'done' to proceed anyway: \")\n",
        "                    if user_input.lower() == 'done':\n",
        "                        upload_done = True\n",
        "            except:\n",
        "                upload_done = True\n",
        "        \n",
        "        images_file = next((k for k in all_uploaded_files if ('image' in k.lower() or 'x_' in k.lower())), None)\n",
        "        labels_file = next((k for k in all_uploaded_files if ('label' in k.lower() or 'y_' in k.lower())), None)\n",
        "        if images_file is None or labels_file is None:\n",
        "            print(\"‚ùå Please include both images and labels (files named x_[...] and y_[...]).\")\n",
        "            sys.exit()\n",
        "        X_all = np.load(images_file)\n",
        "        y_all = np.load(labels_file)\n",
        "    else:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        X_all = np.load(combined_images_path)\n",
        "        y_all = np.load(combined_labels_path)\n",
        "\n",
        "    # Ask for split ratios if needed\n",
        "    print(\"\\nüîÄ Performing stratified 60/20/20 split (train/val/test)\")\n",
        "    X_temp, X_test_raw, y_temp, y_test_raw = train_test_split(\n",
        "        X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
        "    )\n",
        "    X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
        "    )\n",
        "else:\n",
        "    if upload_method == \"upload\":\n",
        "        print(\"üì§ Upload all your files (train/val/test images and labels). You can click 'Choose Files' multiple times to upload from different locations.\")\n",
        "        print(\"Expected file naming: files should contain 'train'/'val'/'test' and 'image'/'x_' for images, 'label'/'y_' for labels\")\n",
        "        \n",
        "        # Allow multiple uploads\n",
        "        upload_done = False\n",
        "        while not upload_done:\n",
        "            try:\n",
        "                uploaded = files.upload()\n",
        "                all_uploaded_files.extend(list(uploaded.keys()))\n",
        "                print(f\"üì• Files uploaded so far: {all_uploaded_files}\")\n",
        "                \n",
        "                # Check what we have\n",
        "                train_x = next((k for k in all_uploaded_files if ('train' in k.lower() and ('image' in k.lower() or 'x_' in k.lower()))), None)\n",
        "                train_y = next((k for k in all_uploaded_files if ('train' in k.lower() and ('label' in k.lower() or 'y_' in k.lower()))), None)\n",
        "                val_x = next((k for k in all_uploaded_files if ('val' in k.lower() and ('image' in k.lower() or 'x_' in k.lower()))), None)\n",
        "                val_y = next((k for k in all_uploaded_files if ('val' in k.lower() and ('label' in k.lower() or 'y_' in k.lower()))), None)\n",
        "                test_x = next((k for k in all_uploaded_files if ('test' in k.lower() and ('image' in k.lower() or 'x_' in k.lower()))), None)\n",
        "                test_y = next((k for k in all_uploaded_files if ('test' in k.lower() and ('label' in k.lower() or 'y_' in k.lower()))), None)\n",
        "                \n",
        "                found_splits = []\n",
        "                if has_train_split and train_x and train_y: found_splits.append(\"train\")\n",
        "                if has_val_split and val_x and val_y: found_splits.append(\"val\")\n",
        "                if has_test_split and test_x and test_y: found_splits.append(\"test\")\n",
        "                \n",
        "                needed_splits = []\n",
        "                if has_train_split: needed_splits.append(\"train\")\n",
        "                if has_val_split: needed_splits.append(\"val\")\n",
        "                if has_test_split: needed_splits.append(\"test\")\n",
        "                \n",
        "                print(f\"‚úÖ Found splits: {found_splits}\")\n",
        "                print(f\"üìã Need splits: {needed_splits}\")\n",
        "                \n",
        "                if set(found_splits) >= set(needed_splits):\n",
        "                    user_input = input(\"‚úÖ All required files found. Type 'done' to proceed or press Enter to upload more files: \")\n",
        "                    if user_input.lower() == 'done':\n",
        "                        upload_done = True\n",
        "                else:\n",
        "                    missing = set(needed_splits) - set(found_splits)\n",
        "                    user_input = input(f\"‚è≥ Still missing: {list(missing)}. Press Enter to upload more files or type 'done' to proceed anyway: \")\n",
        "                    if user_input.lower() == 'done':\n",
        "                        upload_done = True\n",
        "            except:\n",
        "                upload_done = True\n",
        "        \n",
        "        # Load the files\n",
        "        if has_train_split:\n",
        "            xfile = next((k for k in all_uploaded_files if ('train' in k.lower() and ('image' in k.lower() or 'x_' in k.lower()))), None)\n",
        "            yfile = next((k for k in all_uploaded_files if ('train' in k.lower() and ('label' in k.lower() or 'y_' in k.lower()))), None)\n",
        "            if xfile and yfile:\n",
        "                X_train_raw = np.load(xfile)\n",
        "                y_train_raw = np.load(yfile)\n",
        "        \n",
        "        if has_val_split:\n",
        "            xfile = next((k for k in all_uploaded_files if ('val' in k.lower() and ('image' in k.lower() or 'x_' in k.lower()))), None)\n",
        "            yfile = next((k for k in all_uploaded_files if ('val' in k.lower() and ('label' in k.lower() or 'y_' in k.lower()))), None)\n",
        "            if xfile and yfile:\n",
        "                X_val_raw = np.load(xfile)\n",
        "                y_val_raw = np.load(yfile)\n",
        "        \n",
        "        if has_test_split:\n",
        "            xfile = next((k for k in all_uploaded_files if ('test' in k.lower() and ('image' in k.lower() or 'x_' in k.lower()))), None)\n",
        "            yfile = next((k for k in all_uploaded_files if ('test' in k.lower() and ('label' in k.lower() or 'y_' in k.lower()))), None)\n",
        "            if xfile and yfile:\n",
        "                X_test_raw = np.load(xfile)\n",
        "                y_test_raw = np.load(yfile)\n",
        "    \n",
        "    else:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        if has_train_split:\n",
        "            X_train_raw = np.load(train_images_path)\n",
        "            y_train_raw = np.load(train_labels_path)\n",
        "        \n",
        "        if has_val_split:\n",
        "            X_val_raw = np.load(val_images_path)\n",
        "            y_val_raw = np.load(val_labels_path)\n",
        "        \n",
        "        if has_test_split:\n",
        "            X_test_raw = np.load(test_images_path)\n",
        "            y_test_raw = np.load(test_labels_path)\n",
        "\n",
        "    # If val not provided but train/test were, split train to create val\n",
        "    if has_train_split and not has_val_split and X_train_raw is not None and y_train_raw is not None:\n",
        "        print(\"\\nüîÄ Creating validation set (20%) from training data with stratification\")\n",
        "        X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
        "            X_train_raw, y_train_raw, test_size=0.2, random_state=42, stratify=y_train_raw\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Time-frame selection for time-lapse\n",
        "# -----------------------------\n",
        "frame_index = 0  #@param {type:\"integer\"}\n",
        "\n",
        "def select_frame(X, frame_idx):\n",
        "    if X is None:\n",
        "        return None\n",
        "    if len(X.shape) == 4:\n",
        "        # (n_samples, time, H, W)\n",
        "        t = X.shape[1]\n",
        "        idx = max(0, min(frame_idx, t-1))\n",
        "        return X[:, idx, :, :]\n",
        "    return X\n",
        "\n",
        "X_train = select_frame(X_train_raw, frame_index)\n",
        "X_val = select_frame(X_val_raw, frame_index)\n",
        "X_test = select_frame(X_test_raw, frame_index)\n",
        "\n",
        "if not has_augmented_train_set:\n",
        "    print(\"\\nüîÑ Balancing and augmenting TRAIN set...\")\n",
        "    X_train_aug, y_train_aug = augment_dataset(\n",
        "        X_train, y_train_raw, augmentations, augment_times=augment_times, seed=42\n",
        "    )\n",
        "else:\n",
        "    X_train_aug, y_train_aug = X_train, y_train_raw\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Summary and checks\n",
        "# -----------------------------\n",
        "print(\"\\nüìä **Dataset Summary (post-processing):**\")\n",
        "print(f\"   ‚Ä¢ Train: {None if X_train_aug is None else X_train_aug.shape}\")\n",
        "print(f\"   ‚Ä¢ Val:   {None if X_val is None else X_val.shape}\")\n",
        "print(f\"   ‚Ä¢ Test:  {None if X_test is None else X_test.shape}\")\n",
        "\n",
        "# Balance check\n",
        "if y_train_aug is not None:\n",
        "    c0, c1 = np.sum(y_train_aug == 0), np.sum(y_train_aug == 1)\n",
        "    print(f\"\\nüè∑Ô∏è  Train labels ‚Äî Class 0: {c0}, Class 1: {c1}\")\n",
        "    print(\"   ‚Ä¢ Balanced:\" , \"Yes\" if c0 == c1 else \"No\")\n",
        "\n",
        "# Normalisation check across combined datasets (augmented train + val + test)\n",
        "all_sets = []\n",
        "if X_train_aug is not None: all_sets.append(X_train_aug)\n",
        "if X_val is not None: all_sets.append(X_val)\n",
        "if X_test is not None: all_sets.append(X_test)\n",
        "\n",
        "if len(all_sets) > 0:\n",
        "    combined_max = max([arr.max() for arr in all_sets])\n",
        "    combined_min = min([arr.min() for arr in all_sets])\n",
        "    print(f\"\\nüîé Normalisation check ‚Äî global range before: [{combined_min:.3f}, {combined_max:.3f}]\")\n",
        "    if combined_max > 1.0 or combined_min < 0.0:\n",
        "        print(\"‚ö†Ô∏è  Images not in [0,1]. Applying global normalisation across splits...\")\n",
        "        global_max = combined_max if combined_max > 0 else 1.0\n",
        "        if X_train_aug is not None: X_train_aug = X_train_aug / global_max\n",
        "        if X_val is not None: X_val = X_val / global_max\n",
        "        if X_test is not None: X_test = X_test / global_max\n",
        "        combined_max = max([arr.max() for arr in [a for a in [X_train_aug, X_val, X_test] if a is not None]])\n",
        "        combined_min = min([arr.min() for arr in [a for a in [X_train_aug, X_val, X_test] if a is not None]])\n",
        "        print(f\"   ‚Ä¢ New global range: [{combined_min:.3f}, {combined_max:.3f}]\")\n",
        "    else:\n",
        "        print(\"‚úÖ Images already in [0,1].\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Save final variables with correct names for training\n",
        "# -----------------------------\n",
        "# Ensure all final variables are correctly assigned for all cases\n",
        "x_train_aug = X_train_aug if X_train_aug is not None else None\n",
        "y_train_aug = y_train_aug if y_train_aug is not None else None\n",
        "x_val = X_val if X_val is not None else None\n",
        "y_val = y_val_raw if y_val_raw is not None else None\n",
        "x_test = X_test if X_test is not None else None\n",
        "y_test = y_test_raw if y_test_raw is not None else None\n",
        "\n",
        "print(\"\\n‚úÖ Data upload, splitting, augmentation and normalisation completed!\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Final variable summary\n",
        "# -----------------------------\n",
        "print(\"\\nüìã **Final Variables Saved:**\")\n",
        "if x_train_aug is not None:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'x_train_aug': {x_train_aug.shape}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'x_train_aug': None\")\n",
        "    \n",
        "if y_train_aug is not None:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'y_train_aug': {y_train_aug.shape}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'y_train_aug': None\")\n",
        "    \n",
        "if x_val is not None:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'x_val': {x_val.shape}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'x_val': None\")\n",
        "    \n",
        "if y_val is not None:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'y_val': {y_val.shape}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'y_val': None\")\n",
        "    \n",
        "if x_test is not None:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'x_test': {x_test.shape}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'x_test': None\")\n",
        "    \n",
        "if y_test is not None:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'y_test': {y_test.shape}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Variable saved as 'y_test': None\")\n",
        "\n",
        "print(\"\\nüéØ **Ready for AI4CellFate training!**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìê **Image Size Requirements & Model Adaptation**\n",
        "\n",
        "### **Default Configuration**\n",
        "AI4CellFate is designed for **20x20 pixel grayscale images**. If your data matches this size, you can proceed directly to the next cell.\n",
        "\n",
        "### **For Different Image Sizes**\n",
        "If your images are larger (e.g., 64x64, 128x128), you'll need to modify the model architecture:\n",
        "\n",
        "#### **Step-by-Step Guide:**\n",
        "\n",
        "1. **Navigate to Model Files:**\n",
        "   ```\n",
        "   AI4CellFate/\n",
        "   ‚îî‚îÄ‚îÄ src/\n",
        "       ‚îî‚îÄ‚îÄ models/\n",
        "           ‚îú‚îÄ‚îÄ encoder.py    ‚Üê Modify this\n",
        "           ‚îî‚îÄ‚îÄ decoder.py    ‚Üê Modify this\n",
        "   ```\n",
        "\n",
        "2. **Update Encoder Architecture (`src/models/encoder.py`):**\n",
        "   - Locate the `Conv2D` layers in the `__init__` method\n",
        "   - **Add more layers** for larger images:\n",
        "     - For 64x64: Add 1-2 additional Conv2D layers\n",
        "     - For 128x128: Add 2-3 additional Conv2D layers\n",
        "   - **Pattern**: Each Conv2D layer typically halves the spatial dimensions\n",
        "   - **Example**: 128x128 ‚Üí 64x64 ‚Üí 32x32 ‚Üí 16x16 ‚Üí 8x8 ‚Üí 4x4 ‚Üí flatten\n",
        "\n",
        "3. **Update Decoder Architecture (`src/models/decoder.py`):**\n",
        "   - Mirror the encoder changes in reverse\n",
        "   - Adjust the `Dense` layer input size to match encoder output\n",
        "   - Use `Conv2DTranspose` layers to upscale back to original size\n",
        "\n",
        "4. **Key Parameters to Adjust:**\n",
        "   - `filters`: Number of feature maps (typically 32, 64, 128, 256)\n",
        "   - `kernel_size`: Usually (3,3) or (4,4)\n",
        "   - `strides`: Usually (2,2) for downsampling/upsampling\n",
        "   - `padding`: Usually 'same' to maintain dimensions\n",
        "\n",
        "#### **Quick Tips:**\n",
        "- **Maintain symmetry**: Encoder downsampling should match decoder upsampling\n",
        "- **Test incrementally**: Start with one additional layer pair\n",
        "- **Monitor memory**: Larger images require more GPU memory\n",
        "- **Adjust batch size**: You may need to reduce `batch_size` for larger images\n",
        "\n",
        "#### **Example Modification:**\n",
        "For 64x64 images, add one more Conv2D layer in encoder:\n",
        "```python\n",
        "# In encoder.py, add after existing Conv2D layers:\n",
        "x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)  # 64‚Üí32\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "```\n",
        "\n",
        "And corresponding Conv2DTranspose in decoder:\n",
        "```python\n",
        "# In decoder.py, add before existing Conv2DTranspose layers:\n",
        "x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(x)  # 32‚Üí64\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ‚öôÔ∏è **Model Configuration** {display-mode: \"form\"}\n",
        "#@markdown Configure AI4CellFate training parameters\n",
        "\n",
        "# Training parameters\n",
        "latent_dim = 2 #@param {type:\"integer\"}\n",
        "batch_size = 30 #@param {type:\"integer\"}\n",
        "stage1_epochs = 35 #@param {type:\"integer\"}\n",
        "stage2_epochs = 100 #@param {type:\"integer\"}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "random_seed = 42 #@param {type:\"integer\"}\n",
        "\n",
        "# Advanced parameters\n",
        "gaussian_noise_std = 0.003 #@param {type:\"number\"}\n",
        "lambda_recon_stage1 = 5 #@param {type:\"integer\"}\n",
        "lambda_adv_stage1 = 1 #@param {type:\"integer\"}\n",
        "lambda_recon_stage2 = 6 #@param {type:\"integer\"}\n",
        "lambda_adv_stage2 = 4 #@param {type:\"integer\"}\n",
        "lambda_cov = 1 #@param {type:\"number\"}\n",
        "lambda_contra = 8 #@param {type:\"integer\"}\n",
        "\n",
        "# Configuration dictionaries\n",
        "config_stage1 = {\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': stage1_epochs,\n",
        "    'learning_rate': learning_rate,\n",
        "    'seed': random_seed,\n",
        "    'latent_dim': latent_dim,\n",
        "    'GaussianNoise_std': gaussian_noise_std,\n",
        "    'lambda_recon': lambda_recon_stage1,\n",
        "    'lambda_adv': lambda_adv_stage1,\n",
        "}\n",
        "\n",
        "config_stage2 = {\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': stage2_epochs,\n",
        "    'learning_rate': learning_rate,\n",
        "    'seed': random_seed,\n",
        "    'latent_dim': latent_dim,\n",
        "    'GaussianNoise_std': gaussian_noise_std,\n",
        "    'lambda_recon': lambda_recon_stage2,\n",
        "    'lambda_adv': lambda_adv_stage2,\n",
        "    'lambda_cov': lambda_cov,\n",
        "    'lambda_contra': lambda_contra,\n",
        "}\n",
        "\n",
        "print(f\"‚öôÔ∏è **Model Configuration:**\")\n",
        "print(f\"   ‚Ä¢ Latent dimensions: {latent_dim}\")\n",
        "print(f\"   ‚Ä¢ Batch size: {batch_size}\")\n",
        "print(f\"   ‚Ä¢ Stage 1 epochs: {stage1_epochs}\")\n",
        "print(f\"   ‚Ä¢ Stage 2 epochs: {stage2_epochs}\")\n",
        "print(f\"   ‚Ä¢ Learning rate: {learning_rate}\")\n",
        "print(f\"   ‚Ä¢ Random seed: {random_seed}\")\n",
        "print(f\"\\nüîß **Advanced Parameters:**\")\n",
        "print(f\"   ‚Ä¢ Gaussian noise std: {gaussian_noise_std}\")\n",
        "print(f\"   ‚Ä¢ Lambda reconstruction (S1/S2): {lambda_recon_stage1}/{lambda_recon_stage2}\")\n",
        "print(f\"   ‚Ä¢ Lambda adversarial (S1/S2): {lambda_adv_stage1}/{lambda_adv_stage2}\")\n",
        "print(f\"   ‚Ä¢ Lambda covariance: {lambda_cov}\")\n",
        "print(f\"   ‚Ä¢ Lambda contrastive: {lambda_contra}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model configuration completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title üöÄ **AI4CellFate Model Training (Stage 1 + Stage 2)** {display-mode: \"form\"}\n",
        "#@markdown Train the complete AI4CellFate model in two stages: (1) Adversarial Autoencoder, (2) Latent Space Engineering\n",
        "\n",
        "print(\"üöÄ **AI4CellFate Model Training**\")\n",
        "print(\"   Training will proceed in two stages:\")\n",
        "print(\"   ‚Ä¢ Stage 1: Adversarial Autoencoder (reconstruction + Gaussian latent space)\")\n",
        "print(\"   ‚Ä¢ Stage 2: Latent Space Engineering (+ covariance + contrastive losses)\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STAGE 1: Adversarial Autoencoder Training\n",
        "# =============================================================================\n",
        "print(\"üîÑ **Stage 1: Training Adversarial Autoencoder**\")\n",
        "print(\"   Learning basic image reconstruction and Gaussian latent space...\")\n",
        "\n",
        "stage1_results = train_autoencoder(\n",
        "    config_stage1, \n",
        "    x_train_aug, \n",
        "    x_val\n",
        ")\n",
        "\n",
        "# Extract trained models\n",
        "encoder = stage1_results['encoder']\n",
        "decoder = stage1_results['decoder']\n",
        "discriminator = stage1_results['discriminator']\n",
        "\n",
        "print(\"\\n‚úÖ **Stage 1 Completed!**\")\n",
        "print(f\"   ‚Ä¢ Final reconstruction loss: {stage1_results['recon_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final adversarial loss: {stage1_results['adv_loss'][-1]:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STAGE 2: AI4CellFate Training (Latent Space Engineering)\n",
        "# =============================================================================\n",
        "print(\"\\nüîÑ **Stage 2: AI4CellFate Training (Latent Space Engineering)**\")\n",
        "print(\"   Adding covariance and contrastive losses for feature disentanglement...\")\n",
        "\n",
        "stage2_results = train_cellfate(\n",
        "    config_stage2,\n",
        "    encoder,\n",
        "    decoder, \n",
        "    discriminator,\n",
        "    x_train_aug,\n",
        "    y_train_aug,\n",
        "    x_val,\n",
        "    y_val,\n",
        "    x_test,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "# Update models with final trained versions\n",
        "encoder = stage2_results['encoder']\n",
        "decoder = stage2_results['decoder']\n",
        "discriminator = stage2_results['discriminator']\n",
        "final_confusion_matrix = stage2_results['confusion_matrix']\n",
        "\n",
        "print(\"\\nüéâ **Stage 2 Completed!**\")\n",
        "print(f\"   ‚Ä¢ Final reconstruction loss: {stage2_results['recon_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final adversarial loss: {stage2_results['adv_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final covariance loss: {stage2_results['cov_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Final contrastive loss: {stage2_results['contra_loss'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Training stopped at epochs: {stage2_results['good_conditions_stop']}\")\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL RESULTS SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\nüèÜ **Final Training Results:**\")\n",
        "print(\"   ‚Ä¢ Models trained successfully on both stages\")\n",
        "print(\"   ‚Ä¢ Latent space engineered for optimal feature disentanglement\")\n",
        "print(\"   ‚Ä¢ Ready for latent space visualization and interpretation\")\n",
        "\n",
        "# Display final confusion matrix\n",
        "if final_confusion_matrix is not None:\n",
        "    print(f\"\\nüìä **Final Classification Performance:**\")\n",
        "    print(f\"   ‚Ä¢ Class 0 accuracy: {final_confusion_matrix[0,0]:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Class 1 accuracy: {final_confusion_matrix[1,1]:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Mean diagonal accuracy: {np.mean(np.diag(final_confusion_matrix)):.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ **AI4CellFate training completed successfully!**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ **Understanding Your Results**\n",
        "\n",
        "### **Latent Space Visualization**\n",
        "- **Good separation**: Classes should form distinct clusters\n",
        "- **Low correlation**: Features should be independent (correlation < 0.3)\n",
        "- **Centroid distance**: Higher values indicate better class separation\n",
        "\n",
        "### **Feature Interpretation**\n",
        "- **Feature 0 & 1**: Each typically controls different cellular properties\n",
        "- **Common patterns**: Size vs. intensity, morphology vs. signal activity\n",
        "- **Biological relevance**: Features should relate to known cell fate determinants\n",
        "\n",
        "### **Performance Metrics**\n",
        "- **Accuracy**: Overall classification performance\n",
        "- **Precision**: Ability to avoid false positives\n",
        "- **Recall**: Ability to identify all positive cases\n",
        "- **F1-Score**: Balanced measure combining precision and recall\n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Troubleshooting**\n",
        "\n",
        "### **Common Issues**\n",
        "1. **Poor separation**: Try adjusting lambda values or increasing training epochs\n",
        "2. **High correlation**: Increase covariance loss weight (lambda_cov)\n",
        "3. **Low accuracy**: Check data quality, normalization, or try different latent dimensions\n",
        "4. **Overfitting**: Reduce model complexity or increase regularization\n",
        "\n",
        "### **Parameter Tuning**\n",
        "- **Latent dimensions**: Start with 2-3, increase if performance plateaus\n",
        "- **Lambda values**: Balance reconstruction, adversarial, covariance, and contrastive losses\n",
        "- **Training epochs**: Monitor convergence, stop when losses stabilize\n",
        "- **Batch size**: Adjust based on dataset size and available memory\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ **Citation**\n",
        "\n",
        "If you use AI4CellFate in your research, please cite:\n",
        "\n",
        "```\n",
        "[Your paper citation here]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Developed by [Your Name/Lab]** | **üìß Contact: [your.email@institution.edu]** | **üîó GitHub: [repository_link]**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
